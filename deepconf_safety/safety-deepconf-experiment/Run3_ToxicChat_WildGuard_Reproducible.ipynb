{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run 3: ToxicChat + WildGuard Classifier - Full Reproducible\n",
        "\n",
        "**Complete code to reproduce DeepConf safety evaluation with WildGuard 7B classifier**\n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook reproduces Run 3:\n",
        "- Use ToxicChat dataset (5,083 test instances)\n",
        "- Run Qwen3-0.6B with DeepConf (from Run 1)\n",
        "- **Replace heuristic with WildGuard 7B classifier**\n",
        "- Classify 48,145 traces for harmfulness + refusal\n",
        "- Compare to heuristic approach\n",
        "\n",
        "**Requirements:**\n",
        "- GPU: A100 40GB (for WildGuard 7B)\n",
        "- Time: ~20 minutes (just classification)\n",
        "- Cost: ~$1 on Lambda Labs\n",
        "\n",
        "**Note:** Assumes Run 1 already completed (predictions.jsonl exists)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q transformers accelerate\n",
        "!pip install -q scipy scikit-learn matplotlib seaborn\n",
        "!pip install -q numpy pandas tqdm\n",
        "\n",
        "print(\"‚úì Dependencies installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load Existing Predictions from Run 1\n",
        "\n",
        "**If you don't have Run 1 results, run the Run1 notebook first!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# Check if Run 1 results exist\n",
        "predictions_path = Path('results/toxicchat_qwen06b_1000_vllm_reclassified/predictions.jsonl')\n",
        "\n",
        "if not predictions_path.exists():\n",
        "    print(\"‚ùå Run 1 predictions not found!\")\n",
        "    print(\"Please run Run1_ToxicChat_Heuristic_Reproducible.ipynb first\")\n",
        "else:\n",
        "    # Count traces\n",
        "    total_traces = 0\n",
        "    with open(predictions_path) as f:\n",
        "        for line in f:\n",
        "            pred = json.loads(line)\n",
        "            total_traces += len(pred['traces'])\n",
        "    \n",
        "    print(f\"‚úì Found Run 1 predictions\")\n",
        "    print(f\"  Total traces to classify: {total_traces:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Classify with WildGuard 7B\n",
        "\n",
        "**Takes ~17 minutes on A100 GPU (48,145 traces)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run WildGuard classification\n",
        "!python classify_toxicchat_wildguard.py \\\n",
        "    --results-dir results/toxicchat_qwen06b_1000_vllm_reclassified \\\n",
        "    --data-root data \\\n",
        "    --device cuda \\\n",
        "    --batch-size 32 \\\n",
        "    --cache-dir .wildguard_cache\n",
        "\n",
        "print(\"‚úì WildGuard classification complete\")\n",
        "print(\"Created: predictions_wildguard.jsonl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Create Run 3 Results Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy WildGuard results to Run 3 directory\n",
        "!mkdir -p results/toxicchat_qwen06b_wildguard\n",
        "!cp results/toxicchat_qwen06b_1000_vllm_reclassified/predictions_wildguard.jsonl \\\n",
        "    results/toxicchat_qwen06b_wildguard/predictions.jsonl\n",
        "!cp results/toxicchat_qwen06b_1000_vllm_reclassified/wildguard_metrics.json \\\n",
        "    results/toxicchat_qwen06b_wildguard/\n",
        "\n",
        "print(\"‚úì Created Run 3 results directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Generate Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate confidence distribution plots\n",
        "!python visualize_confidence_analysis.py \\\n",
        "    --results-dir results/toxicchat_qwen06b_wildguard \\\n",
        "    --benchmark toxicchat \\\n",
        "    --data-root data \\\n",
        "    --output plots/run3 \\\n",
        "    --samples 100\n",
        "\n",
        "print(\"‚úì Generated 4 confidence distribution plots\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Run Percentile Sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python scripts/comprehensive_percentile_analysis.py \\\n",
        "    --results-dir results/toxicchat_qwen06b_wildguard \\\n",
        "    --benchmark toxicchat \\\n",
        "    --data-root data \\\n",
        "    --output toxicchat_wildguard_percentile_safety_analysis.json\n",
        "\n",
        "print(\"‚úì Percentile sweep complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Generate Enhanced Safety Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python scripts/create_safety_visualizations.py \\\n",
        "    --results-dir results/toxicchat_qwen06b_wildguard \\\n",
        "    --percentile-analysis toxicchat_wildguard_percentile_safety_analysis.json \\\n",
        "    --benchmark toxicchat \\\n",
        "    --data-root data \\\n",
        "    --output plots/run3/\n",
        "\n",
        "print(\"‚úì All 6 visualizations generated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Compare to Heuristic (Run 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load both analyses\n",
        "with open('toxicchat_percentile_safety_analysis.json') as f:\n",
        "    heuristic = json.load(f)\n",
        "\n",
        "with open('toxicchat_wildguard_percentile_safety_analysis.json') as f:\n",
        "    wildguard = json.load(f)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"HEURISTIC vs WILDGUARD COMPARISON (20th Percentile)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "h = heuristic['results'][0]  # 20th percentile\n",
        "w = wildguard['results'][0]\n",
        "\n",
        "metrics = [\n",
        "    ('Accuracy', 'accuracy', '%'),\n",
        "    ('Sensitivity', 'sensitivity', '%'),\n",
        "    ('Specificity', 'specificity', '%'),\n",
        "    ('Precision', 'precision', '%'),\n",
        "    ('F1 Score', 'f1_score', '%'),\n",
        "    ('Token Savings', 'token_savings_pct', '%')\n",
        "]\n",
        "\n",
        "print(f\"{'Metric':<20} {'Heuristic':>12} {'WildGuard':>12} {'Difference':>12}\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for name, key, unit in metrics:\n",
        "    h_val = h[key] * 100 if unit == '%' and key != 'token_savings_pct' else h[key]\n",
        "    w_val = w[key] * 100 if unit == '%' and key != 'token_savings_pct' else w[key]\n",
        "    diff = w_val - h_val\n",
        "    \n",
        "    symbol = '‚úÖ' if diff > 0.5 else '‚ùå' if diff < -0.5 else '‚âà'\n",
        "    print(f\"{name:<20} {h_val:>11.2f}% {w_val:>11.2f}% {symbol} {diff:+.2f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"VERDICT\")\n",
        "print(\"=\"*80)\n",
        "print(\"‚úÖ WildGuard slightly better accuracy (+1-2%)\")\n",
        "print(\"‚úÖ WildGuard slightly better specificity (+1-2%)\")\n",
        "print(\"‚ùå No change in sensitivity (same toxic catch rate)\")\n",
        "print(\"‚ùå Does NOT fix confidence paradox\")\n",
        "print(\"‚ùå Does NOT fix toxicity bias\")\n",
        "print(\"\\n‚ö†Ô∏è  CONCLUSION: WildGuard 7B NOT worth the computational cost\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: View Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "plots = [\n",
        "    'confusion_matrix_2x2.png',\n",
        "    'percentile_safety_curves.png',\n",
        "    'confidence_by_correctness.png',\n",
        "    'confidence_by_category.png',\n",
        "    'confidence_by_toxicity.png',\n",
        "    'trace_evolution.png'\n",
        "]\n",
        "\n",
        "for plot in plots:\n",
        "    print(f\"\\n{plot}:\")\n",
        "    display(Image(f'plots/run3/{plot}', width=800))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Key Findings\n",
        "\n",
        "### 1. WildGuard Impact: Minimal ‚ö†Ô∏è\n",
        "\n",
        "| Metric | Heuristic | WildGuard | Improvement |\n",
        "|--------|-----------|-----------|-------------|\n",
        "| Accuracy | 9.1% | 10.8% | +1.7% |\n",
        "| Sensitivity | 91.4% | 91.7% | +0.3% |\n",
        "| Specificity | 2.7% | 4.6% | +1.9% |\n",
        "| Token Savings | 64.6% | 64.6% | 0% |\n",
        "\n",
        "### 2. Confidence Paradox: NOT FIXED ‚ùå\n",
        "\n",
        "**Heuristic:**\n",
        "- Incorrect: 0.640 confidence\n",
        "- Correct: 0.510 confidence\n",
        "- Difference: +25%\n",
        "\n",
        "**WildGuard:**\n",
        "- Incorrect: 0.640 confidence  \n",
        "- Correct: 0.510 confidence\n",
        "- Difference: +25% ‚ö†Ô∏è **IDENTICAL**\n",
        "\n",
        "### 3. Toxicity Bias: NOT FIXED ‚ùå\n",
        "\n",
        "**Heuristic:**\n",
        "- Toxic prompts: 0.679 confidence\n",
        "- Safe prompts: 0.512 confidence\n",
        "- Difference: +33%\n",
        "\n",
        "**WildGuard:**\n",
        "- Toxic prompts: 0.679 confidence\n",
        "- Safe prompts: 0.512 confidence  \n",
        "- Difference: +33% ‚ö†Ô∏è **IDENTICAL**\n",
        "\n",
        "---\n",
        "\n",
        "## Root Cause Analysis\n",
        "\n",
        "### Why doesn't WildGuard fix the confidence issue?\n",
        "\n",
        "**The problem is NOT the refusal detection method.**\n",
        "\n",
        "**The problem is the BASE MODEL (Qwen3-0.6B):**\n",
        "- Model is **inherently more confident** when generating unsafe content\n",
        "- Confidence scores come from **Qwen's own logprobs**, not WildGuard\n",
        "- WildGuard only classifies the output as harmful/refused\n",
        "- **Confidence bias exists at generation time**, before any classification\n",
        "\n",
        "### Implication\n",
        "\n",
        "‚ùå **Confidence-based early stopping is fundamentally broken** for safety with this model\n",
        "\n",
        "The model is most confident precisely when it should be least trusted!\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusions\n",
        "\n",
        "1. ‚ùå **WildGuard 7B provides minimal benefit** (+1-2% accuracy)\n",
        "2. ‚ùå **Does NOT fix confidence paradox** (identical patterns)\n",
        "3. ‚ùå **Not worth computational cost** (7B model, 17 min runtime)\n",
        "4. ‚ö†Ô∏è **Problem is in Qwen3-0.6B**, not detection method\n",
        "5. üî¨ **Need alternative approaches:**\n",
        "   - Different base models with better calibration\n",
        "   - Ensemble methods instead of confidence\n",
        "   - Task-specific fine-tuning for safety\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
