{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToxicChat Confidence Analysis - Results Viewer\n",
    "\n",
    "**Dataset:** ToxicChat (5,083 instances, 48,145 traces)  \n",
    "**Model:** Qwen3-0.6B  \n",
    "**Date:** November 23, 2024\n",
    "\n",
    "This notebook displays pre-generated visualizations and analysis results.  \n",
    "**No data files needed** - just upload the 4 PNG files from `plots/` directory.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### If running in Colab:\n",
    "\n",
    "1. Upload these 4 plot files to Colab:\n",
    "   - `confidence_by_correctness.png`\n",
    "   - `confidence_by_category.png`\n",
    "   - `confidence_by_toxicity.png`\n",
    "   - `trace_evolution.png`\n",
    "\n",
    "2. Run the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display, Markdown\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# If running in Colab, files should be in /content/\n",
    "# If running locally, adjust path as needed\n",
    "import os\n",
    "plot_dir = '/content/' if os.path.exists('/content/') else 'plots/'\n",
    "\n",
    "print(\"‚úì Setup complete\")\n",
    "print(f\"Looking for plots in: {plot_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Executive Summary\n",
    "\n",
    "## Critical Findings\n",
    "\n",
    "### 1. Confidence is INVERSELY Related to Correctness üö®\n",
    "\n",
    "| Prediction Type | Mean Confidence | Standard Deviation |\n",
    "|----------------|-----------------|-------------------|\n",
    "| **Incorrect** | **0.640** | 0.223 |\n",
    "| **Correct** | **0.510** | 0.215 |\n",
    "\n",
    "**The model is overconfident when it's wrong!**\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Higher Confidence on Toxic Prompts ‚ö†Ô∏è\n",
    "\n",
    "| Ground Truth | Mean Confidence | Num Traces |\n",
    "|--------------|-----------------|------------|\n",
    "| **Toxic Prompts** | **0.679** | 3,414 |\n",
    "| **Safe Prompts** | **0.512** | 44,731 |\n",
    "\n",
    "**The model is MORE confident when responding to toxic content!**\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Uncertain Compliance is Actually Better ‚úÖ\n",
    "\n",
    "| Category | Count | Accuracy | Mean Confidence |\n",
    "|----------|-------|----------|----------------|\n",
    "| **Uncertain Compliance** | **734** | **95.8%** | **0.197** |\n",
    "| Confident Compliance | 4,338 | 88.4% | 0.577 |\n",
    "| Confident Refusal | 9 | 66.7% | 0.745 |\n",
    "| Uncertain Refusal | 2 | 0.0% | 0.116 |\n",
    "\n",
    "**Low-confidence compliance has the BEST accuracy!**\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Refusal Rate is Abysmal üî¥\n",
    "\n",
    "- Total refusals: **11 out of 5,083 instances** (0.2%)\n",
    "- Toxic prompts in dataset: **362** (7.1%)\n",
    "- **Refusal rate on toxic content: 3.0%**\n",
    "- **97% of toxic prompts got responses!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percentile Sweep Results\n",
    "\n",
    "## Lower Percentiles are BETTER!\n",
    "\n",
    "After fixing the `any()` bug with majority voting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentile sweep data\n",
    "data = {\n",
    "    'Percentile': [20, 30, 40, 50, 60, 70, 80, 90],\n",
    "    'Accuracy (%)': [83.04, 83.04, 82.29, 81.21, 80.05, 78.30, 77.75, 77.59],\n",
    "    'Avg Traces': [3.00, 3.00, 4.02, 5.28, 6.29, 8.04, 8.70, 9.07],\n",
    "    'Token Savings (%)': [70.0, 70.0, 60.1, 47.0, 36.4, 19.4, 12.8, 9.1],\n",
    "    'Cost Reduction': ['7.0x', '7.0x', '5.6x', '4.2x', '3.1x', '1.9x', '1.6x', '1.4x']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Style the dataframe\n",
    "styled_df = df.style.apply(\n",
    "    lambda x: ['background-color: lightgreen' if i < 2 else '' for i in range(len(x))],\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "display(df)\n",
    "\n",
    "print(\"\\n‚úÖ Best: 20-30th percentile\")\n",
    "print(\"   - 83.04% accuracy (+5.5% vs 90th)\")\n",
    "print(\"   - 3 traces (vs 9.07)\")\n",
    "print(\"   - 70% token savings (7x cheaper)\")\n",
    "print(\"\\n‚ùå Worst: 90th percentile (original default)\")\n",
    "print(\"   - 77.59% accuracy\")\n",
    "print(\"   - 9.07 traces\")\n",
    "print(\"   - 9.1% token savings (1.4x cheaper)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize accuracy vs efficiency trade-off\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy vs Percentile\n",
    "axes[0].plot(data['Percentile'], data['Accuracy (%)'], \n",
    "             marker='o', linewidth=2.5, markersize=10, color='#2E86AB')\n",
    "axes[0].axhline(y=83.04, color='green', linestyle='--', linewidth=2, alpha=0.7, label='Best (20-30th)')\n",
    "axes[0].axhline(y=77.59, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Default (90th)')\n",
    "axes[0].fill_between(data['Percentile'], 77.59, 83.04, alpha=0.2, color='green')\n",
    "axes[0].set_xlabel('Confidence Percentile Threshold', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Accuracy vs Percentile\\n(Higher percentile = WORSE accuracy)', \n",
    "                  fontsize=13, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].set_ylim([76, 84])\n",
    "\n",
    "# Token Savings vs Percentile\n",
    "axes[1].plot(data['Percentile'], data['Token Savings (%)'], \n",
    "             marker='s', linewidth=2.5, markersize=10, color='#06A77D')\n",
    "axes[1].fill_between(data['Percentile'], data['Token Savings (%)'], \n",
    "                      alpha=0.3, color='#06A77D')\n",
    "axes[1].set_xlabel('Confidence Percentile Threshold', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Token Savings (%)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Token Savings vs Percentile\\n(Lower percentile = MORE savings)', \n",
    "                  fontsize=13, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].set_ylim([0, 75])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display(Markdown(\"\"\"\n",
    "### The \"Overthinking Penalty\"\n",
    "\n",
    "**Why more traces = worse accuracy:**\n",
    "1. Incorrect predictions have **higher confidence** (0.640)\n",
    "2. More traces = more chances for high-confidence wrong answers\n",
    "3. Majority voting can still be swayed by confident errors\n",
    "4. Confidence stays stable across traces (no new information)\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Visualization 1: Confidence by Correctness\n",
    "\n",
    "**Key Finding:** Incorrect predictions have HIGHER confidence!\n",
    "\n",
    "This matches the DeepConf paper style but shows the **opposite** pattern:\n",
    "- DeepConf: Correct predictions had higher confidence ‚úÖ\n",
    "- Our results: Incorrect predictions have higher confidence ‚ùå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=f'{plot_dir}confidence_by_correctness.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistics:**\n",
    "- Correct predictions: mean=0.510, std=0.215\n",
    "- Incorrect predictions: mean=0.640, std=0.223\n",
    "\n",
    "**Implication:** High confidence is a **warning sign**, not a quality signal!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Visualization 2: Confidence by Refusal Category\n",
    "\n",
    "**Key Finding:** Uncertain compliance has the BEST accuracy (95.8%)!\n",
    "\n",
    "Four categories based on confidence threshold (0.5) and refusal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=f'{plot_dir}confidence_by_category.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Category Statistics:**\n",
    "\n",
    "| Category | Mean Conf | Count (traces) | % of Total | Accuracy |\n",
    "|----------|-----------|----------------|------------|----------|\n",
    "| Confident Refusal | 0.745 | 86 | 0.2% | 66.7% |\n",
    "| **Confident Compliance** | **0.577** | **41,228** | **85.7%** | **88.4%** |\n",
    "| **Uncertain Compliance** | **0.197** | **6,815** | **14.2%** | **95.8%** |\n",
    "| Uncertain Refusal | 0.116 | 16 | 0.03% | 0.0% |\n",
    "\n",
    "**Critical Insight:** When the model is uncertain, it makes FEWER mistakes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Visualization 3: Confidence by Ground Truth Toxicity\n",
    "\n",
    "**Key Finding:** Model has HIGHER confidence when responding to toxic prompts!\n",
    "\n",
    "This is dangerous - the model doesn't \"know\" when it should refuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=f'{plot_dir}confidence_by_toxicity.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistics:**\n",
    "- Safe prompts: mean=0.512, std=0.213 (44,731 traces)\n",
    "- Toxic prompts: mean=0.679, std=0.240 (3,414 traces)\n",
    "\n",
    "**Implication:** High confidence does NOT mean safe. In fact, it may indicate the model is confidently responding to something harmful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Visualization 4: Trace Evolution\n",
    "\n",
    "**Key Finding:** Confidence stays STABLE across all 10 traces.\n",
    "\n",
    "Shows 100 sampled instances broken down by correctness √ó toxicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=f'{plot_dir}trace_evolution.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pattern Analysis:**\n",
    "- **Correct on Safe:** Flat ~0.50 (stable, many instances)\n",
    "- **Correct on Toxic:** Slight increase 0.70 ‚Üí 0.85 (only 4 instances!)\n",
    "- **Incorrect on Safe:** Flat ~0.45 (stable, few instances)\n",
    "- **Incorrect on Toxic:** Flat ~0.80 (high confidence, wrong)\n",
    "\n",
    "**Implication:**\n",
    "- Additional traces don't change confidence much\n",
    "- Early stopping at 3 traces is sufficient\n",
    "- More traces just add noise and false positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Cost-Benefit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost calculations\n",
    "num_instances = 5083\n",
    "gpu_rate = 1.29  # $/hour (Lambda Labs)\n",
    "\n",
    "approaches = pd.DataFrame({\n",
    "    'Approach': ['Original (90th)', 'Optimal (20th)', 'Improvement'],\n",
    "    'Accuracy': ['77.59%', '83.04%', '+5.45%'],\n",
    "    'Avg Traces': [9.07, 3.00, '-67%'],\n",
    "    'GPU Hours': [4.0, 1.3, '-67%'],\n",
    "    'Cost (5K instances)': ['$5.16', '$1.68', '-$3.48'],\n",
    "    'Cost (1M/month)': ['$1,014', '$330', '-$684/month']\n",
    "})\n",
    "\n",
    "display(approaches)\n",
    "\n",
    "display(Markdown(\"\"\"\n",
    "### Annual Savings at Scale\n",
    "\n",
    "For 1M queries/month:\n",
    "- **Original cost:** $1,014/month √ó 12 = $12,168/year\n",
    "- **Optimal cost:** $330/month √ó 12 = $3,960/year\n",
    "- **üí∞ Savings: $8,208/year** \n",
    "\n",
    "**And you get 5.5% BETTER accuracy!**\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Hypothesis Validation\n",
    "\n",
    "## Original Hypothesis ‚ùå\n",
    "> \"Low-confidence non-refusals are riskier and should trigger additional scrutiny\"\n",
    "\n",
    "## What We Found ‚úÖ\n",
    "\n",
    "1. **Low-confidence compliance has 95.8% accuracy** (BEST category)\n",
    "2. **High-confidence compliance has 88.4% accuracy** (MORE errors)\n",
    "3. **Low confidence signals careful, accurate responses**\n",
    "4. **High confidence often means the model is wrong**\n",
    "\n",
    "## Revised Understanding\n",
    "\n",
    "**Confidence is an EFFICIENCY signal, NOT a safety signal:**\n",
    "\n",
    "### What Confidence CAN Do ‚úÖ\n",
    "- Enable early stopping (3 traces vs 10)\n",
    "- Reduce costs by 70%\n",
    "- Improve accuracy by avoiding \"overthinking penalty\"\n",
    "\n",
    "### What Confidence CANNOT Do ‚ùå\n",
    "- Predict whether a response is harmful\n",
    "- Identify when the model is wrong (inverse relationship!)\n",
    "- Signal that a toxic prompt should be refused\n",
    "\n",
    "### For Actual Safety\n",
    "- Need WildGuard or similar 7B classifier\n",
    "- Keyword detection misses subtle toxicity\n",
    "- Qwen3-0.6B refuses <3% of toxic content\n",
    "- Confidence alone won't fix this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Recommendations\n",
    "\n",
    "## 1. Use 20-30th Percentile (Not 90th) ‚úÖ\n",
    "\n",
    "**Benefits:**\n",
    "- 5.5% better accuracy (83% vs 77.6%)\n",
    "- 70% token savings (3 traces vs 9)\n",
    "- 7x cost reduction\n",
    "- Avoids \"overthinking penalty\"\n",
    "\n",
    "## 2. The Bug MUST Be Fixed üêõ\n",
    "\n",
    "**Original code (line 346):**\n",
    "```python\n",
    "final_prediction = 'unsafe' if any(self.is_unsafe_content(t) for t in traces) else 'safe'\n",
    "```\n",
    "\n",
    "**Fixed code:**\n",
    "```python\n",
    "unsafe_count = sum(1 for t in traces if self.is_unsafe_content(t))\n",
    "final_prediction = 'unsafe' if unsafe_count >= len(traces) * 0.5 else 'safe'\n",
    "```\n",
    "\n",
    "**Impact:** +12% accuracy improvement\n",
    "\n",
    "## 3. Reframe the Hypothesis üîÑ\n",
    "\n",
    "**Old:** \"Low-confidence non-refusals are riskier\"  \n",
    "**New:** \"Low-confidence enables efficient early stopping\"\n",
    "\n",
    "This is an **efficiency technique**, not a safety technique.\n",
    "\n",
    "## 4. For Safety, Add WildGuard Classifier üõ°Ô∏è\n",
    "\n",
    "**Why:**\n",
    "- Qwen3-0.6B barely refuses (11/5083 instances)\n",
    "- Need proper 7B safety classifier\n",
    "- Confidence alone doesn't predict toxicity\n",
    "- WildGuard can detect subtle refusals and harmfulness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Confidence predicts the opposite of what we expected**\n",
    "   - ‚ùå Higher confidence ‚Üí More errors\n",
    "   - ‚ùå Toxic prompts ‚Üí Higher confidence  \n",
    "   - ‚úÖ Uncertain compliance ‚Üí Best accuracy (95.8%)\n",
    "\n",
    "2. **Lower percentiles are objectively better**\n",
    "   - ‚úÖ 20-30th: 83% accuracy, 70% savings, 7x cheaper\n",
    "   - ‚ùå 90th: 77.6% accuracy, 9% savings, 1.4x cheaper\n",
    "   - The \"overthinking penalty\" is real\n",
    "\n",
    "3. **Confidence is for efficiency, not safety**\n",
    "   - ‚úÖ Use for early stopping to reduce costs\n",
    "   - ‚ùå Don't use for safety filtering\n",
    "   - Need WildGuard for actual toxicity detection\n",
    "\n",
    "4. **Qwen3-0.6B doesn't refuse enough**\n",
    "   - Only 3% refusal rate on toxic content\n",
    "   - 97% of toxic prompts got responses\n",
    "   - Confidence alone won't fix this\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- ‚úÖ Bug fixed and validated (majority voting)\n",
    "- ‚úÖ Percentile sweep complete\n",
    "- ‚úÖ Comprehensive visualizations created\n",
    "- üîÑ Run WildGuard classifier for better safety detection\n",
    "- üîÑ Test on WildGuardMix with gold-standard labels\n",
    "- üìä Statistical hypothesis testing\n",
    "\n",
    "---\n",
    "\n",
    "**Generated:** November 23, 2024  \n",
    "**Experiment:** toxicchat_qwen06b_1000_vllm_reclassified  \n",
    "**Analysis:** DeepConf Confidence-Based Early Stopping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
