{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToxicChat Confidence Distribution Analysis\n",
    "\n",
    "**Dataset:** ToxicChat test set (5,083 instances, ~48K traces)  \n",
    "**Model:** Qwen3-0.6B  \n",
    "**Experiment:** DeepConf with majority voting fix\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This notebook presents a comprehensive analysis of confidence distributions in the ToxicChat safety evaluation experiment.\n",
    "\n",
    "### Critical Findings\n",
    "\n",
    "1. **Confidence is INVERSELY related to correctness** - Incorrect predictions have higher confidence (0.640) than correct ones (0.510)\n",
    "2. **Higher confidence on toxic prompts** - Model is more confident when responding to toxic content (0.679 vs 0.512)\n",
    "3. **Uncertain compliance is better** - 95.7% accuracy vs 88.4% for confident compliance\n",
    "4. **Lower percentiles work better** - 20th percentile: 83% accuracy with 70% token savings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from IPython.display import Image, display, Markdown\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"‚úì Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in Colab, uncomment and run this to mount Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# If running locally, use relative paths\n",
    "results_dir = Path('results/toxicchat_qwen06b_1000_vllm_reclassified')\n",
    "plots_dir = Path('plots')\n",
    "\n",
    "# Load predictions\n",
    "predictions_path = results_dir / 'predictions.jsonl'\n",
    "predictions = []\n",
    "with predictions_path.open('r') as f:\n",
    "    for line in f:\n",
    "        predictions.append(json.loads(line))\n",
    "\n",
    "# Load metrics\n",
    "metrics_path = results_dir / 'reclassification_metrics.json'\n",
    "with metrics_path.open('r') as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "print(f\"‚úì Loaded {len(predictions)} predictions\")\n",
    "print(f\"‚úì Overall accuracy: {metrics['overall_accuracy']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall metrics\n",
    "print(\"=\"*60)\n",
    "print(\"OVERALL METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Predictions:  {metrics['total_predictions']:,}\")\n",
    "print(f\"Correct:            {metrics['correct_predictions']:,}\")\n",
    "print(f\"Accuracy:           {metrics['overall_accuracy']:.1%}\")\n",
    "print()\n",
    "\n",
    "# Category breakdown\n",
    "print(\"=\"*60)\n",
    "print(\"CATEGORY BREAKDOWN\")\n",
    "print(\"=\"*60)\n",
    "for cat, data in metrics['categories'].items():\n",
    "    print(f\"\\n{cat.replace('_', ' ').title()}:\")\n",
    "    print(f\"  Count:      {data['count']:,}\")\n",
    "    print(f\"  Correct:    {data['correct']:,}\")\n",
    "    print(f\"  Accuracy:   {data['accuracy']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Confidence Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth for correctness analysis\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "from benchmark_loaders import ToxicChatLoader\n",
    "\n",
    "instances = ToxicChatLoader.load('data/toxicchat', split='test')\n",
    "ground_truth = {inst.id: inst.ground_truth_label for inst in instances}\n",
    "\n",
    "# Analyze confidences\n",
    "correct_confs = []\n",
    "incorrect_confs = []\n",
    "safe_confs = []\n",
    "toxic_confs = []\n",
    "\n",
    "for pred in predictions:\n",
    "    is_correct = pred['final_prediction'] == ground_truth.get(pred['instance_id'])\n",
    "    gt_label = ground_truth.get(pred['instance_id'])\n",
    "    confs = pred['confidences']\n",
    "    \n",
    "    if is_correct:\n",
    "        correct_confs.extend(confs)\n",
    "    else:\n",
    "        incorrect_confs.extend(confs)\n",
    "    \n",
    "    if gt_label == 'safe':\n",
    "        safe_confs.extend(confs)\n",
    "    elif gt_label == 'unsafe':\n",
    "        toxic_confs.extend(confs)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CONFIDENCE BY CORRECTNESS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Correct predictions:   mean={np.mean(correct_confs):.3f}, std={np.std(correct_confs):.3f}\")\n",
    "print(f\"Incorrect predictions: mean={np.mean(incorrect_confs):.3f}, std={np.std(incorrect_confs):.3f}\")\n",
    "print(f\"\\n‚ö†Ô∏è  INCORRECT predictions have HIGHER confidence!\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CONFIDENCE BY GROUND TRUTH TOXICITY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Safe prompts:  mean={np.mean(safe_confs):.3f}, std={np.std(safe_confs):.3f}, n={len(safe_confs)}\")\n",
    "print(f\"Toxic prompts: mean={np.mean(toxic_confs):.3f}, std={np.std(toxic_confs):.3f}, n={len(toxic_confs)}\")\n",
    "print(f\"\\n‚ö†Ô∏è  TOXIC prompts get HIGHER confidence responses!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Visualizations\n",
    "\n",
    "## 1. Confidence by Correctness\n",
    "\n",
    "**Key Finding:** Incorrect predictions have **higher** confidence than correct ones!\n",
    "\n",
    "- Incorrect: mean = 0.640\n",
    "- Correct: mean = 0.510\n",
    "\n",
    "This is the **opposite** of what we'd expect. The model is overconfident when wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='plots/confidence_by_correctness.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Confidence by Refusal Category\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "1. **Uncertain Compliance (14.2% of traces)** - Mean: 0.197, Accuracy: **95.7%**\n",
    "2. **Confident Compliance (85.7% of traces)** - Mean: 0.577, Accuracy: **88.4%**\n",
    "3. **Refusals are extremely rare** - Only 102 refusals out of 48,145 traces (0.2%)\n",
    "\n",
    "**Implication:** Low-confidence compliance is actually MORE accurate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='plots/confidence_by_category.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Confidence by Ground Truth Toxicity\n",
    "\n",
    "**Key Finding:** The model has **higher confidence** when responding to toxic prompts!\n",
    "\n",
    "- Safe prompts: mean = 0.512\n",
    "- Toxic prompts: mean = 0.679\n",
    "\n",
    "**Implication:** Confidence doesn't signal safety. High confidence may indicate the model is responding to something it shouldn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='plots/confidence_by_toxicity.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Trace Evolution Across Multiple Generations\n",
    "\n",
    "**Key Finding:** Confidence stays **remarkably stable** across all 10 traces.\n",
    "\n",
    "**Implications:**\n",
    "- Additional traces don't add new information\n",
    "- Early stopping at 3 traces (20th percentile) is sufficient\n",
    "- More traces = more opportunities for high-confidence errors\n",
    "\n",
    "**Note:** Only 4 instances where model correctly handled toxic prompts (out of 362 toxic prompts)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='plots/trace_evolution.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Percentile Sweep Results\n",
    "\n",
    "Analysis of different confidence percentile thresholds for early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentile sweep data from ANALYSIS_RESULTS.md\n",
    "percentile_data = {\n",
    "    'Percentile': [20, 30, 40, 50, 60, 70, 80, 90],\n",
    "    'Accuracy': [83.04, 83.04, 82.29, 81.21, 80.05, 78.30, 77.75, 77.59],\n",
    "    'Avg Traces': [3.00, 3.00, 4.02, 5.28, 6.29, 8.04, 8.70, 9.07],\n",
    "    'Token Savings': [70.0, 70.0, 60.1, 47.0, 36.4, 19.4, 12.8, 9.1],\n",
    "    'Cost Reduction': ['7.0x', '7.0x', '5.6x', '4.2x', '3.1x', '1.9x', '1.6x', '1.4x']\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(percentile_data)\n",
    "display(df.style.highlight_max(subset=['Accuracy', 'Token Savings'], color='lightgreen')\n",
    "                 .highlight_min(subset=['Avg Traces'], color='lightgreen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy vs efficiency trade-off\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy vs Percentile\n",
    "axes[0].plot(percentile_data['Percentile'], percentile_data['Accuracy'], \n",
    "             marker='o', linewidth=2, markersize=8, color='blue')\n",
    "axes[0].axhline(y=83.04, color='green', linestyle='--', alpha=0.5, label='Best (20-30th)')\n",
    "axes[0].axhline(y=77.59, color='red', linestyle='--', alpha=0.5, label='Default (90th)')\n",
    "axes[0].set_xlabel('Confidence Percentile Threshold')\n",
    "axes[0].set_ylabel('Accuracy (%)')\n",
    "axes[0].set_title('Accuracy vs Percentile Threshold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "# Token Savings vs Percentile\n",
    "axes[1].plot(percentile_data['Percentile'], percentile_data['Token Savings'], \n",
    "             marker='s', linewidth=2, markersize=8, color='green')\n",
    "axes[1].fill_between(percentile_data['Percentile'], percentile_data['Token Savings'], \n",
    "                      alpha=0.3, color='green')\n",
    "axes[1].set_xlabel('Confidence Percentile Threshold')\n",
    "axes[1].set_ylabel('Token Savings (%)')\n",
    "axes[1].set_title('Token Savings vs Percentile Threshold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Lower percentiles are BETTER: higher accuracy AND lower cost!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Why Lower Percentiles Work Better\n",
    "\n",
    "## The \"Overthinking Penalty\"\n",
    "\n",
    "**20th percentile (3 traces):**\n",
    "- Accuracy: 83.04%\n",
    "- Cost: 7x cheaper\n",
    "\n",
    "**90th percentile (9 traces):**\n",
    "- Accuracy: 77.59%\n",
    "- Cost: 1.4x cheaper\n",
    "\n",
    "**Why this happens:**\n",
    "\n",
    "1. **Incorrect predictions have higher confidence (0.640)**\n",
    "   - More traces = more high-confidence wrong answers\n",
    "   - Majority voting can still be swayed\n",
    "\n",
    "2. **Confidence stays stable across traces**\n",
    "   - Additional traces don't add new information\n",
    "   - Just add noise and false positives\n",
    "\n",
    "3. **Low-confidence traces are actually good**\n",
    "   - Uncertain compliance: 95.7% accurate\n",
    "   - Confident compliance: 88.4% accurate\n",
    "   - Early stopping preserves good low-confidence predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Hypothesis Validation\n",
    "\n",
    "## Original Hypothesis\n",
    "> \"Low-confidence non-refusals are riskier and should trigger additional scrutiny\"\n",
    "\n",
    "## Reality from Data\n",
    "\n",
    "‚úÖ **Low-confidence compliance has 95.7% accuracy**  \n",
    "‚ùå **High-confidence compliance has 88.4% accuracy**  \n",
    "‚ùå **Low confidence actually signals MORE careful, accurate responses**  \n",
    "\n",
    "## Revised Understanding\n",
    "\n",
    "**Confidence is an efficiency signal, NOT a safety signal:**\n",
    "\n",
    "‚úÖ **For efficiency:** Low confidence allows early stopping without hurting accuracy  \n",
    "‚ùå **For safety:** Confidence doesn't predict toxicity or harmfulness  \n",
    "‚ùå **Counterintuitive:** Higher confidence often means more errors  \n",
    "\n",
    "## What Confidence CAN'T Do\n",
    "\n",
    "- ‚ùå Predict whether a response is harmful\n",
    "- ‚ùå Identify when the model is wrong (inverse relationship!)\n",
    "- ‚ùå Signal that a toxic prompt should be refused\n",
    "\n",
    "## What Confidence CAN Do\n",
    "\n",
    "- ‚úÖ Enable early stopping for efficiency (3 traces vs 10)\n",
    "- ‚úÖ Reduce costs by 70% without hurting accuracy\n",
    "- ‚úÖ Identify when the model is uncertain (but uncertainty = better accuracy!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# The Refusal Problem\n",
    "\n",
    "**Critical Issue:** Qwen3-0.6B barely refuses toxic prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate refusal statistics\n",
    "total_traces = sum(len(p['confidences']) for p in predictions)\n",
    "total_refusals = metrics['categories']['confident_refusal']['count'] + \\\n",
    "                 metrics['categories']['uncertain_refusal']['count']\n",
    "\n",
    "toxic_instances = sum(1 for inst in instances if inst.ground_truth_label == 'unsafe')\n",
    "toxic_traces_expected = toxic_instances * 10  # Each instance has 10 traces\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"REFUSAL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total traces:              {total_traces:,}\")\n",
    "print(f\"Total refusals:            {total_refusals} ({total_refusals/total_traces*100:.2f}%)\")\n",
    "print()\n",
    "print(f\"Toxic prompts in dataset:  {toxic_instances} ({toxic_instances/len(instances)*100:.1f}%)\")\n",
    "print(f\"Expected toxic traces:     {toxic_traces_expected:,}\")\n",
    "print(f\"Actual refusals:           {total_refusals}\")\n",
    "print()\n",
    "print(f\"‚ö†Ô∏è  Refusal rate on toxic content: {total_refusals/toxic_traces_expected*100:.1f}%\")\n",
    "print(f\"‚ö†Ô∏è  Toxic responses leaked:        {(toxic_traces_expected - total_refusals)/toxic_traces_expected*100:.1f}%\")\n",
    "print()\n",
    "print(\"Conclusion: Confidence-based early stopping won't help with safety.\")\n",
    "print(\"           Need WildGuard or similar classifier for proper toxicity detection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Recommendations\n",
    "\n",
    "## 1. Use 20-30th Percentile for Efficiency ‚úÖ\n",
    "\n",
    "- **5.5% better accuracy** (83% vs 77.6%)\n",
    "- **70% token savings** (3 traces vs 9)\n",
    "- **7x cost reduction**\n",
    "- Validated finding despite bug fix\n",
    "\n",
    "## 2. Don't Use Confidence for Safety ‚ùå\n",
    "\n",
    "- Confidence is inversely related to correctness\n",
    "- High confidence on toxic prompts is common\n",
    "- Need separate safety classifier (WildGuard)\n",
    "\n",
    "## 3. Early Stopping is About Speed, Not Safety\n",
    "\n",
    "**Frame as:**\n",
    "- \"Reduce inference cost without hurting performance\"\n",
    "- \"Avoid overthinking penalty with efficient early stopping\"\n",
    "\n",
    "**Don't claim:**\n",
    "- \"Identify risky predictions to filter out\"\n",
    "- \"Use confidence to improve safety\"\n",
    "\n",
    "The data shows these are opposite goals!\n",
    "\n",
    "## 4. Next Steps for Safety\n",
    "\n",
    "- Run WildGuard classifier on these results\n",
    "- Compare WildGuard's confidence to Qwen's confidence\n",
    "- Test on WildGuardMix with gold-standard refusal labels\n",
    "- Statistical hypothesis testing (Chi-square, effect sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost calculations\n",
    "num_instances = 5083\n",
    "gpu_rate = 1.29  # $/hour\n",
    "\n",
    "# Original approach (90th percentile)\n",
    "orig_traces = 9.07\n",
    "orig_hours = 4.0\n",
    "orig_cost = orig_hours * gpu_rate\n",
    "\n",
    "# Optimal approach (20th percentile)\n",
    "opt_traces = 3.00\n",
    "opt_hours = 1.3\n",
    "opt_cost = opt_hours * gpu_rate\n",
    "\n",
    "savings = orig_cost - opt_cost\n",
    "savings_pct = (savings / orig_cost) * 100\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"COST ANALYSIS FOR {num_instances:,} INSTANCES\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"Original approach (90th percentile):\")\n",
    "print(f\"  Avg traces:  {orig_traces:.2f}\")\n",
    "print(f\"  GPU time:    {orig_hours:.1f} hours\")\n",
    "print(f\"  Cost:        ${orig_cost:.2f}\")\n",
    "print(f\"  Accuracy:    77.59%\")\n",
    "print()\n",
    "print(\"Optimal approach (20th percentile):\")\n",
    "print(f\"  Avg traces:  {opt_traces:.2f}\")\n",
    "print(f\"  GPU time:    {opt_hours:.1f} hours\")\n",
    "print(f\"  Cost:        ${opt_cost:.2f}\")\n",
    "print(f\"  Accuracy:    83.04%\")\n",
    "print()\n",
    "print(f\"üí∞ Savings:    ${savings:.2f} ({savings_pct:.0f}% cheaper)\")\n",
    "print(f\"üìà Accuracy:   +5.45 percentage points better!\")\n",
    "print()\n",
    "print(\"At scale (1M queries/month):\")\n",
    "monthly_orig = (orig_cost / num_instances) * 1_000_000\n",
    "monthly_opt = (opt_cost / num_instances) * 1_000_000\n",
    "annual_savings = (monthly_orig - monthly_opt) * 12\n",
    "print(f\"  Original:  ${monthly_orig:,.0f}/month\")\n",
    "print(f\"  Optimal:   ${monthly_opt:,.0f}/month\")\n",
    "print(f\"  üí∞ Annual savings: ${annual_savings:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Confidence predicts the opposite of what we expected**\n",
    "   - Higher confidence ‚Üí More errors\n",
    "   - Toxic prompts ‚Üí Higher confidence\n",
    "   - Uncertain compliance ‚Üí Best accuracy (95.7%)\n",
    "\n",
    "2. **Lower percentiles are objectively better**\n",
    "   - 20-30th: 83% accuracy, 70% savings\n",
    "   - 90th: 77.6% accuracy, 9% savings\n",
    "   - The \"overthinking penalty\" is real\n",
    "\n",
    "3. **Confidence is for efficiency, not safety**\n",
    "   - Use for early stopping to reduce costs\n",
    "   - Don't use for safety filtering\n",
    "   - Need WildGuard for actual toxicity detection\n",
    "\n",
    "4. **Qwen3-0.6B doesn't refuse enough**\n",
    "   - Only 2.8% refusal rate on toxic content\n",
    "   - 97.2% of toxic prompts got responses\n",
    "   - Confidence alone won't fix this\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- ‚úÖ Bug fixed and validated (majority voting)\n",
    "- ‚úÖ Percentile sweep complete\n",
    "- ‚úÖ Comprehensive visualizations created\n",
    "- üîÑ Run WildGuard classifier for better safety detection\n",
    "- üîÑ Test on WildGuardMix with gold-standard labels\n",
    "- üìä Statistical hypothesis testing\n",
    "- üìù Write up findings for publication\n",
    "\n",
    "---\n",
    "\n",
    "**Generated:** November 23, 2024  \n",
    "**Experiment ID:** toxicchat_qwen06b_1000_vllm_reclassified  \n",
    "**Branch:** claude/deepconf-adapter-implementation-01Ej26SYeEfWTu9weEEYSU2a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
