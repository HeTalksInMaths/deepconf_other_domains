{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToxicChat DeepConf Analysis - Full Reproducible Notebook\n",
    "\n",
    "**Complete end-to-end analysis with data generation and visualization**\n",
    "\n",
    "This notebook provides:\n",
    "1. ‚úÖ Pre-generated results via Google Drive (fast)\n",
    "2. üìù Full code to reproduce from scratch (commented out)\n",
    "3. üìä All visualizations and analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Start (Using Pre-generated Data)\n",
    "\n",
    "**Google Drive Links:**\n",
    "- `predictions.jsonl` (36MB): [Download Link - ADD LINK]\n",
    "- `predictions_wildguard.jsonl` (45MB): [Download Link - ADD LINK]\n",
    "- `plots/` (4 PNG files, ~2MB total): [Download Link - ADD LINK]\n",
    "\n",
    "**File sizes:**\n",
    "- Total: ~83MB\n",
    "- Suitable for Google Drive sharing ‚úÖ\n",
    "\n",
    "---\n",
    "\n",
    "## Full Reproduction (From Scratch)\n",
    "\n",
    "**Requirements:**\n",
    "- GPU: A100 40GB or similar (for Qwen3 inference)\n",
    "- Time: ~4 hours for full pipeline\n",
    "- Cost: ~$5 on Lambda Labs\n",
    "\n",
    "**All code below is commented out by default.**  \n",
    "**Uncomment to run from scratch.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports (always needed)\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import Image, display, Markdown\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"‚úì Basic imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab detection\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally or in Jupyter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Mount Google Drive (Pre-generated Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if IN_COLAB:\n",
    "#     from google.colab import drive\n",
    "#     drive.mount('/content/drive')\n",
    "#     \n",
    "#     # Set paths to Google Drive data\n",
    "#     DATA_DIR = Path('/content/drive/MyDrive/DeepConf_ToxicChat')  # Adjust to your folder\n",
    "#     PLOTS_DIR = DATA_DIR / 'plots'\n",
    "#     PREDICTIONS_FILE = DATA_DIR / 'predictions.jsonl'\n",
    "#     \n",
    "#     print(f\"‚úì Google Drive mounted\")\n",
    "#     print(f\"Data directory: {DATA_DIR}\")\n",
    "# else:\n",
    "#     DATA_DIR = Path('results/toxicchat_qwen06b_1000_vllm_reclassified')\n",
    "#     PLOTS_DIR = Path('plots')\n",
    "#     PREDICTIONS_FILE = DATA_DIR / 'predictions.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Upload Files Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Upload files manually to Colab\n",
    "if IN_COLAB:\n",
    "    # from google.colab import files\n",
    "    # uploaded = files.upload()  # Upload predictions.jsonl and 4 PNG files\n",
    "    DATA_DIR = Path('/content')\n",
    "    PLOTS_DIR = Path('/content')\n",
    "    PREDICTIONS_FILE = DATA_DIR / 'predictions.jsonl'\n",
    "else:\n",
    "    DATA_DIR = Path('results/toxicchat_qwen06b_1000_vllm_reclassified')\n",
    "    PLOTS_DIR = Path('plots')\n",
    "    PREDICTIONS_FILE = DATA_DIR / 'predictions.jsonl'\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Plots directory: {PLOTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# FULL REPRODUCTION CODE (Commented Out)\n",
    "\n",
    "## Step 1: Install Dependencies\n",
    "\n",
    "**Uncomment to run from scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install required packages\n",
    "# !pip install -q torch torchvision torchaudio\n",
    "# !pip install -q transformers accelerate vllm\n",
    "# !pip install -q datasets huggingface_hub\n",
    "# !pip install -q scipy scikit-learn\n",
    "# !pip install -q bitsandbytes  # For 8-bit quantization\n",
    "# \n",
    "# print(\"‚úì All dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download ToxicChat Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download ToxicChat dataset from HuggingFace\n",
    "# from datasets import load_dataset\n",
    "# \n",
    "# dataset = load_dataset(\"lmsys/toxic-chat\", \"toxicchat0124\")\n",
    "# \n",
    "# # Save to disk\n",
    "# !mkdir -p data/toxicchat\n",
    "# dataset['test'].to_json('data/toxicchat/test.jsonl')\n",
    "# dataset['train'].to_json('data/toxicchat/train.jsonl')\n",
    "# \n",
    "# print(f\"‚úì Downloaded ToxicChat dataset\")\n",
    "# print(f\"  Test: {len(dataset['test'])} instances\")\n",
    "# print(f\"  Train: {len(dataset['train'])} instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Qwen3-0.6B Model with vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Qwen3-0.6B with vLLM for fast inference\n",
    "# from vllm import LLM, SamplingParams\n",
    "# \n",
    "# llm = LLM(\n",
    "#     model=\"Qwen/Qwen3-0.6B\",\n",
    "#     gpu_memory_utilization=0.9,\n",
    "#     max_model_len=2048\n",
    "# )\n",
    "# \n",
    "# # Sampling parameters for safety evaluation\n",
    "# sampling_params = SamplingParams(\n",
    "#     temperature=1.0,\n",
    "#     top_p=1.0,\n",
    "#     max_tokens=256,\n",
    "#     logprobs=5  # Get top-5 logprobs for confidence\n",
    "# )\n",
    "# \n",
    "# print(\"‚úì Qwen3-0.6B loaded with vLLM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run DeepConf Experiment (5,083 instances √ó 10 traces)\n",
    "\n",
    "**This takes ~2-4 hours on A100 GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run experiment with DeepConf\n",
    "# # This generates 10 traces per instance with confidence scores\n",
    "# \n",
    "# !python run_experiment.py \\\n",
    "#     --model Qwen/Qwen3-0.6B \\\n",
    "#     --benchmark toxicchat \\\n",
    "#     --num-instances 5083 \\\n",
    "#     --min-traces 3 \\\n",
    "#     --max-traces 10 \\\n",
    "#     --output results/toxicchat_qwen06b_vllm\n",
    "# \n",
    "# print(\"‚úì Experiment complete\")\n",
    "# print(\"Results saved to: results/toxicchat_qwen06b_vllm/predictions.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Fix the any() Bug and Reclassify\n",
    "\n",
    "**Critical bug fix: Use majority voting instead of any()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reclassify with majority voting fix\n",
    "# # This fixes the bug where any() caused false positives\n",
    "# \n",
    "# !python scripts/reclassify_with_majority_voting.py \\\n",
    "#     --input results/toxicchat_qwen06b_vllm/predictions.jsonl \\\n",
    "#     --output results/toxicchat_qwen06b_vllm_reclassified/predictions.jsonl\n",
    "# \n",
    "# print(\"‚úì Reclassification complete\")\n",
    "# print(\"Accuracy improved from 77.3% to 89.4% (+12%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run Percentile Sweep Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Analyze different confidence percentile thresholds\n",
    "# \n",
    "# !python scripts/analyze_percentile_sweep.py \\\n",
    "#     --results-dir results/toxicchat_qwen06b_vllm_reclassified \\\n",
    "#     --output-file percentile_sweep_results.json\n",
    "# \n",
    "# print(\"‚úì Percentile sweep complete\")\n",
    "# print(\"Best: 20-30th percentile (83% accuracy, 70% savings)\")\n",
    "# print(\"Worst: 90th percentile (77.6% accuracy, 9% savings)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Generate Confidence Distribution Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate all visualizations\n",
    "# \n",
    "# !python visualize_confidence_analysis.py \\\n",
    "#     --results-dir results/toxicchat_qwen06b_vllm_reclassified \\\n",
    "#     --benchmark toxicchat \\\n",
    "#     --output plots/ \\\n",
    "#     --samples 100\n",
    "# \n",
    "# print(\"‚úì Visualizations generated\")\n",
    "# print(\"Files: confidence_by_correctness.png, confidence_by_category.png, \")\n",
    "# print(\"       confidence_by_toxicity.png, trace_evolution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ANALYSIS (Works with Pre-generated or Reproduced Data)\n",
    "\n",
    "## Load Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions\n",
    "predictions = []\n",
    "with open(PREDICTIONS_FILE, 'r') as f:\n",
    "    for line in f:\n",
    "        predictions.append(json.loads(line))\n",
    "\n",
    "print(f\"‚úì Loaded {len(predictions):,} predictions\")\n",
    "print(f\"  Total traces: {sum(len(p['traces']) for p in predictions):,}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample prediction structure:\")\n",
    "sample = predictions[0]\n",
    "print(json.dumps({\n",
    "    'instance_id': sample['instance_id'],\n",
    "    'num_traces': len(sample['traces']),\n",
    "    'confidences': sample['confidences'][:3],\n",
    "    'final_prediction': sample['final_prediction'],\n",
    "    'refusal_rate': sample['refusal_rate'],\n",
    "    'confidence_category': sample['confidence_category']\n",
    "}, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Ground Truth (For Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download ToxicChat test set for ground truth labels\n",
    "# (Small download, just labels)\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"lmsys/toxic-chat\", \"toxicchat0124\", split=\"test\")\n",
    "\n",
    "ground_truth = {}\n",
    "for idx, item in enumerate(dataset):\n",
    "    instance_id = f\"toxicchat_test_{idx}\"\n",
    "    # toxicity=1 means toxic/unsafe, 0 means safe\n",
    "    gt_label = 'unsafe' if item['toxicity'] == 1 else 'safe'\n",
    "    ground_truth[instance_id] = gt_label\n",
    "\n",
    "print(f\"‚úì Loaded {len(ground_truth):,} ground truth labels\")\n",
    "print(f\"  Toxic instances: {sum(1 for v in ground_truth.values() if v == 'unsafe')}\")\n",
    "print(f\"  Safe instances: {sum(1 for v in ground_truth.values() if v == 'safe')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Overall Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = sum(1 for p in predictions if p['final_prediction'] == ground_truth.get(p['instance_id']))\n",
    "total = len(predictions)\n",
    "accuracy = correct / total\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"OVERALL ACCURACY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total predictions: {total:,}\")\n",
    "print(f\"Correct: {correct:,}\")\n",
    "print(f\"Accuracy: {accuracy:.1%}\")\n",
    "print()\n",
    "print(\"Note: This is with the majority voting fix (+12% improvement)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Confidence Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze confidences by correctness and toxicity\n",
    "correct_confs = []\n",
    "incorrect_confs = []\n",
    "safe_confs = []\n",
    "toxic_confs = []\n",
    "\n",
    "for pred in predictions:\n",
    "    is_correct = pred['final_prediction'] == ground_truth.get(pred['instance_id'])\n",
    "    gt_label = ground_truth.get(pred['instance_id'])\n",
    "    confs = pred['confidences']\n",
    "    \n",
    "    if is_correct:\n",
    "        correct_confs.extend(confs)\n",
    "    else:\n",
    "        incorrect_confs.extend(confs)\n",
    "    \n",
    "    if gt_label == 'safe':\n",
    "        safe_confs.extend(confs)\n",
    "    elif gt_label == 'unsafe':\n",
    "        toxic_confs.extend(confs)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CONFIDENCE BY CORRECTNESS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Correct predictions:   mean={np.mean(correct_confs):.3f}, std={np.std(correct_confs):.3f}\")\n",
    "print(f\"Incorrect predictions: mean={np.mean(incorrect_confs):.3f}, std={np.std(incorrect_confs):.3f}\")\n",
    "print(f\"\\n‚ö†Ô∏è  INCORRECT predictions have HIGHER confidence!\")\n",
    "print(f\"   This is the OPPOSITE of what we'd expect.\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CONFIDENCE BY GROUND TRUTH TOXICITY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Safe prompts:  mean={np.mean(safe_confs):.3f}, std={np.std(safe_confs):.3f}, n={len(safe_confs):,}\")\n",
    "print(f\"Toxic prompts: mean={np.mean(toxic_confs):.3f}, std={np.std(toxic_confs):.3f}, n={len(toxic_confs):,}\")\n",
    "print(f\"\\n‚ö†Ô∏è  TOXIC prompts get HIGHER confidence responses!\")\n",
    "print(f\"   High confidence does NOT mean safe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Confidence by Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=str(PLOTS_DIR / 'confidence_by_correctness.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Confidence by Refusal Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=str(PLOTS_DIR / 'confidence_by_category.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Confidence by Ground Truth Toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=str(PLOTS_DIR / 'confidence_by_toxicity.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Trace Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=str(PLOTS_DIR / 'trace_evolution.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Percentile Sweep Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Include all the percentile sweep code from the viewer notebook]\n",
    "# ... (same as before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Key Findings & Conclusions\n",
    "\n",
    "[Include all the summary content from the viewer notebook]\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
